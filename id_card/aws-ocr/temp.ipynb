{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator, constants\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the Google API translator\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First name/given'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## translate a spanish text to english text (by default)\n",
    "# translation = translator.translate(\"Vornamen/Given\", dest=\"hi\")\n",
    "translation = translator.translate(\"Vornamen/Given\")\n",
    "\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./id_card_imgs/belgium1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, labels, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "    #\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 1]\n",
    "    y1 = boxes[:, 0]\n",
    "    x2 = boxes[:, 3]\n",
    "    y2 = boxes[:, 2]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    final_labels = [labels[idx] for idx in pick]\n",
    "    final_boxes = boxes[pick].astype(\"int\")\n",
    "    return final_boxes, final_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vietocr.tool.translate import build_model, translate, translate_beam_search, batch_translate_beam_search\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(self, config):\n",
    "        device = config['device']\n",
    "\n",
    "        model, vocab = build_model(config)\n",
    "        weights = config['weights']\n",
    "\n",
    "        model.load_state_dict(torch.load(weights, map_location=torch.device(device)))\n",
    "\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def predict(self, img):\n",
    "        img = img / 255.0\n",
    "        img = self.preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = torch.FloatTensor(img)\n",
    "        img = img.to(self.config['device'])\n",
    "\n",
    "        if self.config['predictor']['beamsearch']:\n",
    "            sent = translate_beam_search(img, self.model)\n",
    "            s = sent\n",
    "        else:\n",
    "            s = translate(img, self.model)[0].tolist()\n",
    "\n",
    "        s = self.vocab.decode(s)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def batch_predict(self, images):\n",
    "        \"\"\"\n",
    "        param: images : list of ndarray\n",
    "        \"\"\"\n",
    "        batch_dict, indices = self.batch_process(images)\n",
    "        list_keys = [i for i in batch_dict if batch_dict[i] != batch_dict.default_factory()]\n",
    "        result = list([])\n",
    "\n",
    "        for width in list_keys:\n",
    "            batch = batch_dict[width]\n",
    "            batch = np.asarray(batch)\n",
    "            batch = torch.FloatTensor(batch)\n",
    "            batch = batch.to(self.config['device'])\n",
    "\n",
    "            if self.config['predictor']['beamsearch']:\n",
    "                sent = batch_translate_beam_search(batch, model=self.model)\n",
    "            else:\n",
    "                sent = translate(batch, self.model).tolist()\n",
    "\n",
    "            batch_text = self.vocab.batch_decode(sent)\n",
    "            result.extend(batch_text)\n",
    "\n",
    "        # sort text result to original coordinate\n",
    "        def get_index(element):\n",
    "            return element[1]\n",
    "\n",
    "        z = zip(result, indices)\n",
    "        sorted_result = sorted(z, key=get_index)\n",
    "        result, _ = zip(*sorted_result)\n",
    "\n",
    "        return result\n",
    "\n",
    "     def preprocess_input(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess input image (resize, normalize)\n",
    "\n",
    "        Parameters:\n",
    "        image: has shape of (H, W, C)\n",
    "\n",
    "        Return:\n",
    "        img: has shape (H, W, C)\n",
    "        \"\"\"\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        new_w, image_height = self.resize_v1(w, h, self.config['dataset']['image_height'],\n",
    "                                             self.config['dataset']['image_min_width'],\n",
    "                                             self.config['dataset']['image_max_width'])\n",
    "\n",
    "        img = cv2.resize(image, (new_w, image_height))\n",
    "        img = img / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def batch_process(self, images):\n",
    "        batch_img_dict = defaultdict(list)\n",
    "        image_height = self.config['dataset']['image_height']\n",
    "\n",
    "        batch_img_li = [self.preprocess_input(img) for img in images]\n",
    "        batch_imgs, width_list, indices = self.sort_width(batch_img_li, reverse=False)\n",
    "\n",
    "        min_bucket_width = min(width_list)\n",
    "        max_width = max(width_list)\n",
    "        thresh = 30\n",
    "        max_bucket_width = np.minimum(min_bucket_width + thresh, max_width)\n",
    "\n",
    "        for i, image in enumerate(batch_imgs):\n",
    "            c, h, w = image.shape\n",
    "\n",
    "            # reset min_bucket_width, max_bucket_width\n",
    "            if w > max_bucket_width:\n",
    "                min_bucket_width = w\n",
    "                max_bucket_width = np.minimum(min_bucket_width + thresh, max_width)\n",
    "\n",
    "            avg_bucket_width = int((max_bucket_width + min_bucket_width) / 2)\n",
    "\n",
    "            new_img = self.resize_v2(image, avg_bucket_width, height=image_height)\n",
    "            batch_img_dict[avg_bucket_width].append(new_img)\n",
    "\n",
    "        return batch_img_dict, indices\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_width(batch_img, reverse=False):\n",
    "        def get_img_width(element):\n",
    "            img = element[0]\n",
    "            c, h, w = img.shape\n",
    "            return w\n",
    "\n",
    "        batch = list(zip(batch_img, range(len(batch_img))))\n",
    "        sorted_batch = sorted(batch, key=get_img_width, reverse=reverse)\n",
    "        sorted_batch_img, indices = list(zip(*sorted_batch))\n",
    "\n",
    "        return sorted_batch_img, list(map(get_img_width, batch)), indices\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_v1(w, h, expected_height, image_min_width, image_max_width):\n",
    "        new_w = int(expected_height * float(w) / float(h))\n",
    "        round_to = 10\n",
    "        new_w = math.ceil(new_w / round_to) * round_to\n",
    "        new_w = max(new_w, image_min_width)\n",
    "        new_w = min(new_w, image_max_width)\n",
    "\n",
    "        return new_w, expected_height\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_v2(img, width, height):\n",
    "        new_img = np.transpose(img, (1, 2, 0))\n",
    "        new_img = cv2.resize(new_img, (width, height), cv2.INTER_AREA)\n",
    "        new_img = np.transpose(new_img, (2, 0, 1))\n",
    "\n",
    "        return new_img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('kenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "675f33c4e9571de98fc6278933ab6a04fc7353f622fb2afd99ecefe31f8e8f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
